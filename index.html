<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Pranav Agarwal</title>
  
  <meta name="author" content="Pranav Agarwal">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/icon.png">
  <style>
        #widget-container {
            position: fixed;
            bottom: 0;
            left: 0;
        }
        .timeline {
            display: flex;
            flex-direction: column;
            gap: 10px;
        } 

        .timeline li {
            display: flex;
            flex-direction: row;
            align-items: flex-start;
        }
        .timeline li div:first-child {
        min-width: 100px;
        }
        
        .scrollable-news {
          max-height: 400px; /* Adjust the maximum height as needed */
          overflow-y: scroll;
        }
    </style>
</head>

<body>
  <table style="width:100%;max-width:900px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Pranav Agarwal</name>
              </p>
              <p>I am a Ph.D. candidate at <a href="https://www.etsmtl.ca/">École de technologie supérieure</a> and <a href="https://mila.quebec/en/">Mila</a> working on Deep Reinforcement Learning for robotic applications and character animation. I am fascinated by the potential of leveraging human knowledge to guide reinforcement learning policies for solving complex real-world applications.</p>
              <p> Previously, I was a student researcher at <a href="https://www.inria.fr/en">Inria</a> where I collaborated with <a href="https://sites.google.com/view/nataliadiaz">Natalia Díaz-Rodríguez</a> and
         <a href="https://team.inria.fr/rits/membres/raoul-de-charette/">Raoul de CHARETTE</a>. I completed my Bachelors in Electronics and Communication Engineering at <a href="https://www.iiitg.ac.in/">IIIT Guwahati</a>, where I was awarded the President's Gold Medal. During my bachelor's I worked as a research intern at <a href="https://www.sutd.edu.sg/">SUTD</a> with Professor <a href="https://scholar.google.com/citations?user=6MjMhT4AAAAJ&hl=en">Gemma Roig</a>.
        
              <p style="text-align:center">
                    [
                <a href="mailto:pranav.agarwal.2109@gmail.com">Email</a> &nbsp/&nbsp
                <a href="data/Agarwal_Pranav_CV.pdf">CV</a> &nbsp/&nbsp
                <a href="https://github.com/pranavAL">Github</a> &nbsp/&nbsp
                <a href="https://twitter.com/Pranav_AL">Twitter</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=QFEzapMAAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp
                <a href="https://www.linkedin.com/in/pranav-agarwal-6b4453114//">Linkedin</a>
]
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/prof2.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/prof2.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
        <!-- <h2>Background</h2> -->
        <p> 
        <!-- with <a href="https://webdocs.cs.ualberta.ca/~dale/">Dale Schuurmans</a> and <a href="https://www.afaust.info/">Aleksandra Faust</a> in similar areas. --> 
        <!-- I was fortunate to also work on consumer privacy rights and legislation with folks from <a href="https://www.law.georgetown.edu/privacy-technology-center">Georgetown's Center on Privacy and Technology.</a></p> -->
              </p>
              <p>
              </p>
        <h2>News</h2>
        <div class="scrollable-news">
        <ul class="timeline">
          <li>
            <div><b>Nov 2023</b>:</div>
            <div><b>Reviewer</b> - Served as reviewer for <a href="https://2024.ieee-icra.org/">ICRA</a>.</div>
          </li>
          <li>
            <div><b>Oct 2023</b>:</div>
            <div><b>Paper</b> - Our work <a href="">Empowering Clinicians with MeDT: A Framework for Sepsis Treatment</a> was accepted at <b>Goal-Conditioned Reinforcement Learning Workshop, NeurIPS 2023 (<strong><span style="color: red;">Spotlight</span></strong>)</b>.</div>
          </li>
          <li>
            <div><b>Oct 2023</b>:</div>
            <div><b>Reviewer</b> - Served as reviewer for <a href="https://sice-si.org/SII2024/">SII</a>.</div>
          </li>
          <li>
            <div><b>Sept 2023</b>:</div>
            <div><b>Exam</b> - Passed the Ph.D. proposal exam.</div>
          </li>
          <li>
            <div><b>Jul 2023</b>:</div>
            <div><b>Paper</b> - Preprint release of our work <a href="https://arxiv.org/pdf/2307.05979.pdf"> Transformers in Reinforcement Learning: A Survey</a>.</div>
          </li>
          <li>
            <div><b>Apr 2023</b>:</div>
            <div><b>Reviewer</b> - Served as reviewer for <a href="https://iccv2023.thecvf.com/">ICCV</a> and <a href="https://ieee-iros.org/">IROS</a></b>.</div>
          </li>
          <li>
            <div><b>Dec 2022</b>:</div>
            <div><b>Paper</b> - Our work <a href="https://arxiv.org/pdf/2211.07941.pdf">Automatic Evaluation of Excavator Operators using Learned Reward Functions</a> was accepted at <b>Reinforcement Learning for Real Life Workshop, Neurips 2022</b>.</div>
          </li>
          <li>
              <div><b>Nov 2022</b>:</div>
              <div><b>Reviewer</b> - Served as reviewer for  <a href="https://www.ieee-ras.org/publications/ra-l">Robotics and Automation Letters</a>.</div>
            </li>  
          <li>  
            <div><b>Sept 2022</b>:</div>
            <div><b>Position</b> - Fast-tracked to a PhD position at <a href="https://mila.quebec/">Mila</a>.</div>
          </li>
          <li>
            <div><b>Jun 2022</b>:</div>
            <div><b>Award</b> - Received an Exemption from International Tuition Fees for Graduate studies.</div>
          </li>
          <li>
            <div><b>Jan 2022</b>:</div>
            <div><b>Position</b> - Started Masters by Research at <a href="https://mila.quebec/">Mila</a>, in collaboration with <a href="https://www.cm-labs.com/en/">CM Labs</a>.</div>
          </li>
          <li>
            <div><b>Mar 2020</b>:</div>
            <div><b>Paper</b> - Our work <a href="https://arxiv.org/pdf/2003.11743.pdf">Egoshots</a> was accepted at <b>Machine Learning in Real Life Workshop, ICLR 2020</b>.</div>
          </li>
          <li>
            <div><b>May 2019</b>:</div>
            <div><b>Position</b> - Started a research position at Inria.</div>
          </li>
          <li>
            <div><b>Apr 2019</b>:</div>
            <div><b>Graduation</b> - Graduated from IIIT Guwahati with President's Gold Medal.</div>
          </li>
          <li>
            <div><b>Mar 2019</b>:</div>
            <div><b>Paper</b> - Our work <a href="https://ieeexplore.ieee.org/abstract/document/8971330">Learning to synthesize faces using voice clips for Cross-Modal biometric matching</a> was accepted at IEEE Region 10 Symposium (TENSYMP).</div>
          </li>
          <li>
            <div><b>Mar 2019</b>:</div>
            <div><b>Award</b> - Received the Best Technology Award (reward of 1500 USD) by Govt. of India.</div>
          </li>
          <li>
            <div><b>May 2018</b>:</div>
            <div><b>Position</b> - Started a Research Internship at <a href="https://www.sutd.edu.sg/">Singapore University of Technology and Design</a>.</div>
          </li>
        </ul>
      </div>
            <!--<h2>Research 	&#129302;</h2>-->
        <h2>Research</h2>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

          <tr onmouseout="nightsight_stop()" onmouseover="nightsight_start()" bgcolor="#ffffd0">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/survey-full.gif" alt="elign" style="border-style: none" width="350">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://pranavAL.github.io/transformers-in-rl-survey/">
                <papertitle>Transformers in Reinforcement Learning: A Survey</papertitle>
              </a>
              <br>
              <strong>Pranav Agarwal</strong>,
              <a href="https://aamer98.github.io/">Aamer Abdul Rahman</a>,
              <a href="https://mila.quebec/en/person/plstcharles/">Pierre-Luc St-Charles</a>,
              <a href="https://www.linkedin.com/in/simon-prince-615bb9165/?originalSubdomain=ca">Simon J.D. Prince</a>,
              <a href="https://saebrahimi.github.io/">Samira Ebrahimi Kahou</a>
              <br>
              <em> In submission (2023)</em>. 
              <br>
              [
              <a href="https://arxiv.org/pdf/2307.05979.pdf">Paper</a> 
               ]
              <br>
              <p>
                Transformers have significantly impacted domains like natural language processing, computer vision, and robotics, improving performance compared to other neural networks. This survey explores their use in reinforcement learning (RL), where they address challenges such as unstable training, credit assignment, interpretability, and partial observability. It provides an overview of RL, discusses challenges faced by classical RL algorithms, and examines how transformers are well-suited to tackle these challenges. The survey covers the application of transformers in representation learning, transition and reward function modeling, and policy optimization within RL. It also discusses efforts to enhance interpretability and efficiency through visualization techniques and tailored adaptations for specific applications. Limitations and potential for future breakthroughs are assessed as well.
            </td>
          </tr>



          <tr onmouseout="nightsight_stop()" onmouseover="nightsight_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/medt_eval_gif_title.gif" alt="elign" style="border-style: none" width="350">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="">
                <papertitle>Empowering Clinicians with MeDT: A Framework for Sepsis Treatment</papertitle>
              </a>
              <br>
              <a href="https://aamer98.github.io/">Aamer Abdul Rahman</a>,
              <strong>Pranav Agarwal</strong>,
              <a href="https://vmichals.github.io/">Vincent Michalski</a>,
              <a href="https://www.etsmtl.ca/en/research/professors/rnoumeir/">Rita Noumeir</a>,
              <a href="https://recherche.umontreal.ca/english/our-researchers/professors-directory/researcher/is/in15318/">Philippe Jouvet</a>,
              <a href="https://saebrahimi.github.io/">Samira Ebrahimi Kahou</a>
              <br>
              <em> NeurIPS 2023 Goal-Conditioned Reinforcement Learning Workshop (<strong><span style="color: red;">Spotlight</span></strong>)</em>. 
              <br>
              [
              <a href="">Paper</a> /
              <a href="https://github.com/Aamer98/MeDT">Code</a> /
              <a href="https://aamer98.github.io/medical_decision_transformer/">Webpage</a> /
              <a href="">Slides</a> 
               ]
              <br>
              <p>
                Offline reinforcement learning is promising for safety-critical tasks like clinical decision support, but faces challenges of interpretability and clinician interactivity. To overcome these, the proposed Medical Decision Transformer (MeDT) utilizes a goal-conditioned RL paradigm for sepsis treatment recommendations. MeDT employs the decision transformer architecture, considering factors like treatment outcomes, patient acuity scores, dosages, and current/past medical states to provide a holistic view of the patient's history. This enhances decision-making by allowing MeDT to generate actions based on user-specified goals, ensuring clinician interactability and addressing sparse rewards. Results from the MIMIC-III dataset demonstrate MeDT's effectiveness in producing interventions that either outperform or compete with existing methods, offering a more interpretable, personalized, and clinician-directed approach.
            </td>
          </tr> 


                
          <tr onmouseout="nightsight_stop()" onmouseover="nightsight_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/crane.gif" alt="hpp" style="border-style: none" width="350">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/2211.07941.pdf">
                <papertitle>Automatic Evaluation of Excavator Operators using Learned Reward Functions</papertitle>
              </a>
              <br>
              <strong>Pranav Agarwal</strong>,
              <a href="https://www.linkedin.com/in/marekteichmann/?originalSubdomain=ca">Marek Teichmann</a>,
              <a href="https://profs.etsmtl.ca/sandrews/">Sheldon Andrews</a>,
              <a href="https://saebrahimi.github.io/">Samira Ebrahimi Kahou</a>
              <br>
              <em>NeurIPS 2022 Reinforcement Learning for Real Life Workshop</em>. 
              <br>
              [
              <a href="https://arxiv.org/pdf/2211.07941.pdf">Paper</a> /
              <a href="https://github.com/pranavAL/InvRL_Auto-Evaluate">Code</a> /
              <a href="https://drive.google.com/file/d/1jR1otOAu8zrY8mkhUOUZW9jkBOAKK71Z/view?usp=share_link">Video</a> /
              <a href="https://docs.google.com/presentation/d/1KEXmwOWRN_q6o5lPVk83O8mnfiCV-0kpGrnj2USp1fw/edit?usp=sharing">Slides</a>
               ]
              <p>Training novice users to operate an excavator for learning different skills requires the presence of expert teachers. Considering the complexity of the problem, it is comparatively expensive to find skilled experts as the process is timeconsuming and requires precise focus. Moreover, since humans tend to be biased, the evaluation process is noisy and will lead to high variance in the final score of different operators with similar skills. In this work, we address these
                issues and propose a novel strategy for the automatic evaluation of excavator
                operators. We take into account the internal dynamics of the excavator and the
                safety criterion at every time step to evaluate the performance.
              </p>
            </td>
          </tr>
               

          <tr onmouseout="nightsight_stop()" onmouseover="nightsight_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/carla.gif" alt="kts" style="border-style: none" width="350">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/2103.09189.pdf">
                <papertitle>Goal-constrained Sparse Reinforcement Learning for End-to-End Driving</papertitle>
              </a>
              <br>
              <strong>Pranav Agarwal</strong>,
              <a href="https://www.linkedin.com/in/pierre-de-beaucorps-06064099/">Pierre de Beaucorps</a>,
              <a href="https://team.inria.fr/rits/membres/raoul-de-charette/">Raoul de Charette</a>
              <br>
              <em>In submission (2021)</em>. 
              <br>
              [
              <a href="https://arxiv.org/pdf/2103.09189.pdf">Paper</a> /
              <a href="https://github.com/pranavAL/Goal-constrained-Sparse-Reinforcement-Learning-for-End-to-End-Driving">Code</a> /
              <a href="https://www.youtube.com/watch?v=6mD9OwrAroU">Video</a> 
               ]
              <br>
              <p> Deep reinforcement Learning for end-to-end driving is limited by the need of complex reward engineering. Sparse
                  rewards can circumvent this challenge but suffers from long
                  training time and leads to sub-optimal policy. In this work, we
                  explore full-control driving with only goal-constrained sparse
                  reward and propose a curriculum learning approach for end-toend driving using only navigation view maps that benefit from
                  small virtual-to-real domain gap. To address the complexity of
                  multiple driving policies, we learn concurrent individual policies
                  selected at inference by a navigation system. We demonstrate
                  the ability of our proposal to generalize on unseen road layout,
                  and to drive significantly longer than in the training. 
</p>
            </td>
          </tr>

          <tr onmouseout="nightsight_stop()" onmouseover="nightsight_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/elign.png" alt="elign" style="border-style: none" width="300">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/2003.11743.pdf">
                <papertitle>Egoshots, an ego-vision life-logging dataset and semantic fidelity metric to evaluate diversity in image captioning models</papertitle>
              </a>
              <br>
              <strong>Pranav Agarwal</strong>,
              <a href="https://scholar.google.com/citations?user=fLHBgLMAAAAJ&hl=en">Alejandro Betancourt</a>,
              <a href="https://www.linkedin.com/in/vanapanagiotou/?originalSubdomain=gr">Vana Panagiotou</a>,
              <a href="https://sites.google.com/view/nataliadiaz">Natalia Diaz-Rodriguez</a>
              <br>
              <em>Machine Learning in Real Life (ML-IRL) ICLR 2020 Workshop</em>. 
              <br>
              [
              <a href="https://arxiv.org/pdf/2003.11743.pdf">Paper</a> /
              <a href="https://github.com/pranavAL/Semantic_Fidelity-and-Egoshots">Code</a> /
              <a href="https://www.youtube.com/watch?v=TFzxFfI90sc">Video</a> /
              <a href="https://docs.google.com/presentation/d/1UOadIVy_CYFsFC5POjfgUxvwxST35P6kDXUp2Au4kVI/edit#slide=id.p">Slides</a>
               ]
              <br>
              <p>
               In this paper, we attempt to show the biased nature of the
currently existing image captioning models and present a new image captioning
dataset, Egoshots, consisting of 978 real life images with no captions. We further
exploit the state of the art pre-trained image captioning and object recognition networks to annotate our images and show the limitations of existing works. Furthermore, in order to evaluate the quality of the generated captions, we propose a new
image captioning metric, object based Semantic Fidelity (SF). Existing image captioning metrics can evaluate a caption only in the presence of their corresponding
annotations; however, SF allows evaluating captions generated for images without
annotations, making it highly useful for real life generated captions. 
            </td>
          </tr>

          <tr onmouseout="nightsight_stop()" onmouseover="nightsight_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/clap.png" alt="elign" style="border-style: none" width="350">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://ieeexplore.ieee.org/abstract/document/8971330">
                <papertitle>Learning to synthesize faces using voice clips for Cross-Modal biometric matching</papertitle>
              </a>
              <br>
              <strong>Pranav Agarwal</strong>,
              <a href="https://scholar.google.com/citations?user=_MwJODcAAAAJ&hl=en">Soumyajit Poddar</a>,
              <a href="https://scholar.google.co.in/citations?user=v3U0HZ4AAAAJ&hl=en">Anakhi Hazarika</a>,
              <a href="https://scholar.google.com/citations?user=s73BsKIAAAAJ&hl=en">Hafizur Rahaman</a>
              <br>
              <em> 2019 IEEE Region 10 Symposium (TENSYMP)</em>. 
              <br>
              [
              <a href="https://ieeexplore.ieee.org/abstract/document/8971330">Paper</a> /
              <a href="https://github.com/pranavAL/Cross_modal_Biometric_matching">Code</a>
               ]
              <br>
              <p>
                In this paper, a framework for cross-modal biometric matching is presented, where faces of an individual are generated using his/her voice clips and further the synthesized faces are tested using a face classification network. We explore the advancements of Convolutional Neural Network (CNN) for feature extraction and generative networks for image synthesis. In the experiment, we compare the performance of Variational Autoencoders(VAE), Conditional Generative Adversarial Networks(C-GAN) and Regularized Conditional Generative Adversarial Networks(RC-GAN) and show that RC-GAN that is C-GAN with a regularization factor added to its loss is able to generate faces corresponding to the true identity of the voice clips with the best accuracy of 84.52% while VAE generates a less noise prone image with the highest PSNR of 28.276 decibels but with an accuracy of 72.61%.
            </td>
          </tr>

          
        </tbody></table>

      </td>
    </tr>
  </table>

 <table class = 'about-edu'>
            <tr>
              
              <td align="center" width="16%" style = "vertical-align: middle; background-color: rgba(255, 255, 255, 1)">
                <a href="https://www.cm-labs.com/"><img src = "images/cmlabs.jpeg" width="45%"></a>
              </td>

              <td align="center" width="16%" style = "vertical-align: middle; background-color: rgba(255, 255, 255, 1)">
                <a href="https://mila.quebec/"><img src = "images/mila.webp" width="45%"></a>
              </td>

              <td align="center" width="16%" style = "vertical-align: middle; background-color: rgba(255, 255, 255, 1)">
                <a href="https://www.inria.fr/en"><img src = "images/inria.png" width="45%"></a>
              </td>
              
              <td align="center" width="16%" style = "vertical-align: middle; background-color: rgba(255, 255, 255, 1)">
                <a href="https://www.iiitg.ac.in/"><img src = "images/iitg.jpg" width="45%"></a>
              </td>
              
       
              <td align="center" width="16%" style = "vertical-align: middle; background-color: rgba(255, 255, 255, 1)">
                <a href="https://www.sutd.edu.sg/"><img src = "images/sutd.jpg" width="42%"></a>
              </td>
                            
              <td align="center" width="16%" style = "vertical-align: middle; background-color: rgba(255, 255, 255, 1)">
                <a href="https://iisc.ac.in/"><img src = "images/iisc.jpg" width="42%"></a>
              </td>
              
                            
            </tr>

            <tr>
              
              <td align="center" style = "vertical-align: middle; background-color: rgba(255, 255, 255, 1)">Research Student<br><b>CM-Labs</b><br>Jan 2022</td>

              <td align="center" style = "vertical-align: middle; background-color: rgba(255, 255, 255, 1)">PhD Student<br><b>Mila, Québec</b><br>Jan 2022</td>

               <td align="center" style = "vertical-align: middle; background-color: rgba(255, 255, 255, 1)">Research Assistant<br><b>Inria, Paris</b><br>May 2019 - April 2021</td>
                
                
                <td align="center" style = "vertical-align: middle; background-color: rgba(255, 255, 255, 1)">B.Tech ECE<br><b>IIIT Guwahati</b><br>Aug 2015 - May 2019</td>
                
                <td align="center" style = "vertical-align: middle; background-color: rgba(255, 255, 255, 1)">Research Intern<br><b>Singapore University of Technology and Design</b><br>May 2018 - Aug 2018 </td>
              
                <td align="center" style = "vertical-align: middle; background-color: rgba(255, 255, 255, 1)">Research Intern<br><b>Indian Institute of Science, Bangalore</b><br>May 2017 - Aug 2017</td>

              </tr>
              
          </table>
  
<div style="text-align: center;">
    <div style="display: inline-block; margin-top: 40px;">
        <table class="analytics">
            <tr>
                <script type="text/javascript" id="clustrmaps" src="//cdn.clustrmaps.com/map_v2.js?cl=ffffff&w=150&t=n&d=7GitBdIjM6_HrTVgbGuqjaG9RLy_0UXsikjyC2QZBVQ"></script>
            </tr>
        </table>
    </div>
</div>able>
</div>
  
 </body>

</html>
