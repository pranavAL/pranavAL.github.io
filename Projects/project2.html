<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="Deep Learning Blog">
    <meta name="keywords" content="Deep Learning, AI, Machine Learning, Python">
    <meta name="author" content="Pranav Agarwal">
    <title>MAMBA - Blog</title>
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.28.0/themes/prism-okaidia.min.css" rel="stylesheet" />

    <!-- Link to CSS styles (you can add your own or link to a stylesheet) -->
    <link rel="stylesheet" href="styles.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0-beta3/css/all.min.css">


    <!-- Google Fonts -->
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">

    <!-- MathJax for Rendering Math -->
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

    <style>
        body {
            font-family: 'Roboto', sans-serif;
            line-height: 1.6;
            margin: 0;
            padding: 0;
            background-color: #f4f4f9;
            color: #333;
        }

        .container {
            max-width: 1000px;
            margin: 50px auto;
            padding: 20px;
            background: white;
            box-shadow: 0 4px 8px rgba(0, 0, 0, 0.1);
        }

        h1, h2, h3 {
            color: #333;
            margin-bottom: 20px;
        }

        h1 {
            font-size: 2.5rem;
            color: #1e88e5;
            border-bottom: 2px solid #1e88e5;
            padding-bottom: 10px;
        }

        h2 {
            font-size: 2rem;
            margin-top: 40px;
        }

        p {
            font-size: 1.1rem;
            margin-bottom: 20px;
        }

        code {
            background-color: #f1f1f1 !important;
            padding: 1px !important;
            font-size: 1.2rem !important;
            border-radius: 4px !important;
            color: #000 !important;
            margin-bottom: 0 !important; /* Ensure no extra space at the bottom */
        }
    
        /* Styling for block code inside <pre> */
        pre {
            background-color: #272822 !important;
            color: #f8f8f2 !important;
            padding: 8px !important;
            border-radius: 5px !important;
            margin-bottom: 5px !important; /* Reduce margin at the bottom */
            margin-left: 0 !important;
            padding-left: 10px !important;
            padding-bottom: 1px !important; /* Reduce bottom padding */
            overflow-x: auto !important;
            font-size: 1.25rem !important;
            line-height: 1.4 !important;
            white-space: pre-wrap !important;
        }
    
        pre code {
            background-color: transparent !important;
            color: inherit !important;
            padding-left: 0 !important;
            font-size: inherit !important;
        }

        img {
            max-width: 100%;
            height: auto;
            margin-bottom: 20px;
            border-radius: 10px;
        }

        .math-block {
            background-color: #f9f9f9;
            border-left: 4px solid #007bff;
            padding: 15px;
            margin: 20px 0;
            font-family: Arial, sans-serif;
        }
        .math-block h3 {
            margin-top: 0;
            color: #007bff;
        }
        .math-block .equation {
            display: block;
            text-align: center;
            margin: 10px 0;
            font-size: 1.2em;
        }

        .toc {
            background-color: #e3f2fd;
            padding: 20px;
            border-radius: 10px;
            margin-bottom: 40px;
        }

        .toc ul {
            list-style-type: none;
            padding: 0;
        }

        .toc a {
            text-decoration: none;
            color: #1e88e5;
        }

        .toc a:hover {
            text-decoration: underline;
        }

        .quote {
            font-style: italic;
            color: #555;
            border-left: 4px solid #1e88e5;
            padding-left: 10px;
            margin: 20px 0;
        }

        .footer {
            text-align: center;
            padding: 20px;
            background-color: #1e88e5;
            color: white;
        }

        .footer a {
            color: white;
            text-decoration: underline;
        }

        .footer a:hover {
            text-decoration: none;
        }

        .button {
            background-color: transparent; /* Transparent background */
            color: #1e88e5; /* Text color */
            padding: 12px 25px; /* Increased padding */
            text-align: center;
            border: 2px solid transparent; /* Transparent border */
            border-radius: 25px; /* More rounded edges */
            text-decoration: none;
            position: relative; /* For hover effect */
            overflow: hidden; /* Hide the overflow */
            transition: color 0.4s; /* Smooth color transition */
            font-size: 1rem; /* Base font size */
            display: flex; /* Flexbox for alignment */
            align-items: center; /* Center vertically */
        }
    
        .button::before {
            content: ''; /* Empty content for the hover effect */
            position: absolute; /* Positioning the hover effect */
            top: 0;
            left: 0;
            width: 100%;
            height: 100%;
            background: #1e88e5; /* Hover background */
            transform: scaleX(0); /* Start scaled down */
            transition: transform 0.4s; /* Smooth transition */
            border-radius: 25px; /* Match border radius */
            z-index: 0; /* Send behind the text */
        }
    
        .button:hover::before {
            transform: scaleX(1); /* Expand the hover effect */
        }
    
        .button:hover {
            color: white; /* Change text color on hover */
        }
    
        .button img {
            width: 50px; /* Increased logo size */
            height: auto; 
            margin-right: 10px; /* More space between text and logo */
        }

        .home-button {
            color: #363636;
            text-decoration: none;
            font-size: 1.2rem;
            padding: 0.5rem 1rem;
            border: 2px solid #363636;
            border-radius: 5px;
            transition: all 0.3s ease;
            position: fixed;
            top: 20px; /* Adjust as needed */
            left: 20px; /* Adjust as needed */
          }
          
          .home-button:hover {
            background-color: #363636;
            color: #fff;
          }
          
    </style>
</head>

<body>


    <div class="container">
                <a href="https://pranaval.github.io/" class="home-button">
                  <i class="fas fa-home"></i> Home
                </a>
        <h1>MAMBA: Can We Achieve Infinite Context Length?</h1>
        <p>
Infinite context length is important for Large Language Models (LLMs) as it can significantly enhance their ability to understand and solve complex tasks. With unlimited context, users could provide vast amounts of data of any modalities, 
enabling the model to dynamically select the most relevant information for a given query. This capability could lead to more advanced AI agents that continuously integrate and retain knowledge over extended periods. 
However, achieving infinite context length is challenging due to computational constraints and the need for efficient memory management. These limitations are primarily due to their quadratic complexity of Transformers, with respect to sequence length, making scalability a major hurdle.
        </p>

        <!-- Center-Aligned Embedded Tweet -->
<div style="display: flex; justify-content: center;">
    <blockquote class="twitter-tweet">
        <p lang="en" dir="ltr">
            What would it look like to combine Google search (shallow Knowledge Graph reasoning over an ultra-ultra-wide index) <br><br>
            with LLM in-context learning (highly intelligent operations on a tiny index)?
            <a href="https://t.co/PLh265Bloe">pic.twitter.com/PLh265Bloe</a>
        </p>
        &mdash; Dwarkesh Patel (@dwarkesh_sp) 
        <a href="https://twitter.com/dwarkesh_sp/status/1890337241886580815?ref_src=twsrc%5Etfw">February 14, 2025</a>
    </blockquote>
</div>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

        <h2 id="introduction">1. Are Transformers all we need?</h2>
        <p> 
          Transformers are the go-to-architecture for large language models (LLMs), primarily due to their ability to model long context (reaching millions of tokens) using the
          <strong>self-attention mechanism</strong>. Self-attention enables the model to selectively focus on relevant tokens in the input sequence when generating the output. 
          Mathematically, self-attention computes a weighted sum of values \( V \) based on the similarity between query \( Q \) and keys \( K \):</p>
          <div class="math-block">
            <div class="equation">
            $$ \text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V $$</div>
            </div>
            <p>Here, \( Q, K, V \in \mathbb{R}^{N \times d_k} \) are the query, key, and value matrices, and \( d_k \) is the dimension of the keys, and the 
          softmax ensures that the attention weights sum to 1. </p>

<pre><code class="language-python">
def attention(Q, K, V):
    """
    Computes the scaled dot-product attention.

    Args:
        Q (torch.Tensor): Query tensor of shape (batch_size, seq_len, d_k).
        K (torch.Tensor): Key tensor of shape (batch_size, seq_len, d_k).
        V (torch.Tensor): Value tensor of shape (batch_size, seq_len, d_v).

    Returns:
        torch.Tensor: Output tensor of shape (batch_size, seq_len, d_v).
    """
    d_k = Q.size(-1)  # Dimension of the key vectors
    scores = torch.matmul(Q, K.transpose(-2, -1)) / torch.sqrt(torch.tensor(d_k))  # QK^T / sqrt(d_k)
    weights = F.softmax(scores, dim=-1)  # Softmax over the last dimension
    output = torch.matmul(weights, V)  # Weighted sum of values
    return output    
</code>
</pre>
<p>This mechanism enables efficient parallelization during training because the attention scores for all tokens can be computed in parallel. However, Transformers suffer 
    from quadratic complexity with respect to sequence length \( N \), i.e., \( O(N^2d_k) \). This makes training and inference 
    computationally expensive for long sequences.</p>
    <h3 id="inference">1.1 Inference Bottlenecks and KV Caching</h2>
    <p> Unlike training, where self-attention is computed in parallel across tokens in a sequence, inference in autoregressive models proceeds
        sequentially. The model generates one token at a time, and each new token requires computing attention scores with all the previously generated tokens. This leads to increasing memory 
    and compute costs as the sequence grows.</p>

    <p>To mitigate this, Key-Value (KV) caching is commonly used (including many other strategies) to improve computational efficiency. KV caching stores the attention keys and value from previous time steps,
          allowing the model to reuse them instead of recomputing attention scores from scratch. This reduces redundant computations and 
          significantly speeds up the inference.</p>
<pre>
<code class="language-python">
class KVCache:
    def __init__(self, max_length, d_model):
        """
        Initialize the KV cache.
        """
        self.max_length = max_length
        self.d_model = d_model
        self.keys = torch.empty((0, d_model))  # Initialize empty cache for keys
        self.values = torch.empty((0, d_model))  # Initialize empty cache for values

    def update(self, new_keys, new_values):
        """
        Update the KV cache with new keys and values.
        """
        # Concatenate new keys and values to the cache
        self.keys = torch.cat([self.keys, new_keys], dim=1)  # Concatenate along sequence dimension
        self.values = torch.cat([self.values, new_values], dim=1)

        # Truncate the cache if it exceeds max_length
        if self.keys.size(1) > self.max_length:
            self.keys = self.keys[:, -self.max_length:, :]
            self.values = self.values[:, -self.max_length:, :]

    def get_attention_output(self, query):
        """
        Compute attention output using the cached keys and values.
        """        
        output = attention(query, self.keys, self.values)
        return output

# Example Usage
batch_size = 2
seq_len = 10
d_model = 64
max_length = 128

# Initialize KV cache
kv_cache = KVCache(max_length=max_length, d_model=d_model)

# Dummy input tensors (batch_size=2, seq_len=10, d_model=64)
new_keys = torch.randn(batch_size, seq_len, d_model)
new_values = torch.randn(batch_size, seq_len, d_model)
query = torch.randn(batch_size, seq_len, d_model)

# Update cache with new keys and values
kv_cache.update(new_keys, new_values)

# Compute attention output
output = kv_cache.get_attention_output(query)
</code>
</pre>
<p>KV caching improves inference speed by avoiding redundant computations, making it feasible to handle long sequences. However for very long sequences, the cache may need to be
    pruned or truncated to fit within memory limits.
        </p>
        
        </p>
        <h2 id="RNN">2. Can RNNs help?</h2> 
    <p>Recurrent Neural Networks (RNNs) are a class of neural networks designed for sequential data processing. They maintain a <strong>hidden state</strong> that captures past context and updates it at each time step
        based on the current input and the previous hidden state. Mathematically, the update rule and the output is given by:</p>
        
        <div class="math-block">
            <h3>Hidden State Update:</h3>
            <div class="equation">
                \[
                h(t) = f\big(W_h h(t-1) + W_i x(t)\big)
                \]
            </div>
            <h3>Output:</h3>
            <div class="equation">
                \[
                y(t) = g\big(W_o h(t)\big)
                \]
            </div>
        </div>
        where:
            <ul>
                <li>
                    <p> \( h(t) \) is the hidden state at time \( t \), which serves as memory.</p>
                </li>
                <li>
                    <p>\( x(t) \) is the input at time \( t \).</p>
                </li>
                <li>
                <p> \( W_h \) and \( W_i \) are weight matrices.</p>
                </li>
                <li>
                <p> \( f \) is the activation function (e.g., ReLU, tanh).</p>
                </li>
                <li><p>\( W_o \) is the weight matrix for the output.</p></li>
                <li><p>\( g \) is the output activation function (e.g., softmax for classification).</p></li>
            </ul>
        
        <p>Theoretically, RNNs can model arbitrarily <strong>long sequences</strong>, 
        but in practice, they struggle with <strong>long-range dependencies</strong> due to the 
    <strong>vanishing gradient problem</strong>—gradients shrink as they propagate through time, making it difficult to learn from distant tokens. Also, the fixed size of the given hidden state
makes it difficult to compress long sequences, leading to information loss.</p>  
   
<p><strong>Long Short-Term Memory (LSTM)</strong> and <strong>Gated Recurrent Unit (GRU)</strong> architectures introduced <strong>gating mechanisms</strong> to control the flow of information.
    These gating mechanism allow the model to selectively retain or forget information, improving its ability to capture long-range dependencies. However they still struggle
    with very long range sequences and their sequential nature prevents <strong> parallelization</strong>, making training slow.</p>


        <h2 id="ssm">3. How about State Space Models?</h2>
        <p>
            State Space Models (SSMs) provide an alternative to RNNs by replacing <strong>discrete recurrence</strong> with <strong>structured state transitions</strong>, 
            leading to <strong>faster computation and better long-range memory retention</strong>. SSMs are commonly used in control theory to modeling the dynamics of systems, and have been adopted for sequence modeling.
            <div class="math-block"> 
                <h3>Continuous-Time SSM Equations:</h3>
                <div class="equation">
                    \[
                    \dot{h}(t) = A h(t) + B x(t)
                    \]
                </div>
                <div class="equation">
                    \[
                    y(t) = C h(t) + D x(t)
                    \]
                </div>
                <h3>Discrete-Time SSM Equations:</h3>
                <div class="equation">
                    \[
                    h(t) = \overline{A} h(t-1) + \overline{B} x(t)
                    \]
                </div>
                <div class="equation">
                    \[
                    y(t) = C h(t) + D x(t)
                    \]
                </div>
            </div>
            Where:
            <ul>
                <li><strong>\( h(t) \in \mathbb{R}^d \)</strong> is the <strong>hidden state</strong> at time \( t \), acting as memory.</li>
                <li><strong>\( x(t) \)</strong> is the <strong>input</strong> at time \( t \).</li>
                <li><strong>\( y(t) \)</strong> is the <strong>output</strong>.</li>
                <li><strong>\( A \)</strong> defines the hidden state evolution.</li>
                <li><strong>\( B \)</strong> determines the input influence on the hidden state.</li>
                <li><strong>\( C \)</strong> maps the hidden state to the output.</li>
                <li><strong>\( D \)</strong> allows direct input-output connections.</li>
            </ul>        
        </p>

    <p> Unlike RNNs, SSMs can process sequences in parallel by leveraging <strong>structured matrix multiplication</strong> (e.g., using convolutional or FFT operations), making them significantly faster and more memory efficient. 
        However, SSMs are <strong>time-invariant</strong>, meaning the matrices \( A \), \( B \), \( C \), and \( D \) remain the same for each token. This limits their ability to perform <strong>content-aware reasoning</strong>, 
        such as dynamically focusing on or ignoring specific parts of the input sequence.    </p>
    
        <h2 id="mamba">4. What Makes Mamba Special?</h2>
<p>
    Mamba introduces <strong>Selective State Space Models</strong> and utilizes <strong> hardware optimizations</strong> (<em>surprising fact: MAMBA was rejected at ICLR-2024</em>). 
    It is designed to handle long sequences (like text or time series data) faster and more efficiently than RNNs or Transformers. Mamba stands out because of its:
</p>

<ul>
    <li>
        <p><strong>Selective Focus</strong> — Mamba can <strong>dynamically</strong> select parts of the input to pay attention to. This makes it great at understanding context.</p>
    </li>
    <li>
        <p><strong>Fast Inference</strong> — Unlike Transformers, which use a slow and memory-hungry attention mechanism, Mamba uses a lightweight <strong>State Space Model (SSM)</strong> block. This makes it much faster, especially for long sequences.</p>
    </li>
    <li>
        <p><strong>Hardware-Friendly</strong> — Mamba is designed to work efficiently on modern hardwares. It minimizes memory usage by fusing multiple operations into a single step, reducing the need to constantly read and write data.</p>
    </li>
</ul>

<p>
    In essence, Mamba combines the best of both worlds: the parallel processing capabilities of Transformers with the efficient memory retention of RNNs.
</p>

<h3 id="mamba">4.1 How Mamba Works ?</h3>
<p>
    At its core, Mamba is based on <strong>State Space Models (SSMs)</strong>. However it add selectivity to these models. This enables
    it to dynamically decide which part of the input to focus on and which part to ignore. This selectivity is achieved by making 
    some of the parameters input dependent.
</p>

<h3 id="mamba">4.2 The Math Behind Mamba</h3>
<p>
    Mamba uses two main equations to process data. These equations describe how the hidden state evolves over time and how the output is computed:
</p>

<div class="math-block">
    <h3>Hidden State Update</h4>
    <div class="equation">
    $$h(t) = A h(t-1) + B x(t)$$
</div>
    <p>
        Here:
    </p>
    <ul>
        <li>\( h(t) \) is the <strong>hidden state</strong> at time \( t \). Think of it as the model’s memory.</li>
        <li>\( x(t) \) is the <strong>input</strong> at time \( t \). This could be a word in a sentence or a data point in a time series.</li>
        <li>\( A \) and \( B \) are matrices that control how the hidden state evolves and how the input influences it.</li>
    </ul>
</div>

<div class="math-block">
    <h3>Output Equation</h3>
    <div class="equation">
    $$y(t) = C h(t) + D x(t)$$
</div>
    <p>
        Here:
    </p>
    <ul>
        <li>\( y(t) \) is the <strong>output</strong> at time \( t \). This is the model’s prediction or representation of the input.</li>
        <li>\( C \) and \( D \) are matrices that map the hidden state and input to the output.</li>
    </ul>
</div>



<h3 id="mamba">4.3 Discretization: Making It Work for Sequences</h3>
<p>
    Since real-world data (like text) is discrete (e.g., words or tokens), these continuous-time equations are converted into a form that works for discrete sequences. 
</p>

<p>
    For this <strong>zero-order hold (ZOH)</strong> method is used to discretize the continuous equations. The discretized version is given as:
</p>

<div class="math-block">
    <div class="equation">
    $$A_{\Delta} = e^{A \Delta t}, \quad B_{\Delta} = A^{-1}(e^{A \Delta t} - I)B$$
</div>
    <p>
        Where:
    </p>
    <ul>
        <li>\( A_{\Delta} \) and \( B_{\Delta} \) are the <strong>discretized versions</strong> of \( A \) and \( B \).</li>
        <li>\( \Delta t \) is a learned step size that controls how fast the hidden state evolves.</li>
    </ul>
</div>

<p>
    This discretization step is crucial because it allows Mamba to handle real-world data efficiently.
        Mamba makes \( B \), \( C \), and the step size \( \Delta \) <strong>input-dependent</strong>. This means it can adapt its behavior based on the input, making it context dependent.
</p>

<h3 id="mamba">4.4 Parallel Processing</h3>
<p>
    One of Mamba’s biggest strengths is its ability to process sequences in <strong>parallel</strong>, even though it’s based on a recurrent model. This is achieved using <strong>parallel scan algorithm</strong>.
</p>

<p> Recurrent models compute the hidden state \( h_t \) sequentially, where each state depends on the previous one:</p>
<div class="math-block">
    <div class="equation">
    $$h_t = f(h_{t-1}, x_t),$$
</div>
</div>

<p>
    This sequential dependency makes parallelization challenging. 
    Mamba breaks the sequence into smaller chunks, processes them in parallel, and then combines the results. 
    The key insight is that the state update operation often satisfies the <strong>associative property</strong>, which allows for efficient parallel computation.
    Mathematically, if the state update function \( f \) is associative, then:</p> 
    <div class="math-block">
    <div class="equation">
    $$f(f(h_1, x_2), x_3) = f(h_1, f(x_2, x_3)).$$
    </div>
    </div>
<p>This property enables Mamba to compute partial states independently and combine them later.</p>

<p>
    Mamba leverages this <strong>associative property</strong> of the state update operation to enable parallel processing. Here’s how it works:
</p>

<ol>
    <li>
        <p><strong>Break the Sequence into Chunks</strong> — The input sequence \( X = [x_1, x_2, \dots, x_T] \) is divided into \( K \) smaller chunks \( C_1, C_2, \dots, C_K \), 
            , where each chunk \( C_i \) contains a subset of the sequence. For example, if \( T = 8 \) and \( K = 2 \), the chunks might be \( C_1 = [x_1, x_2, x_3, x_4] \) and \( C_2 = [x_5, x_6, x_7, x_8] \).</p>
    </li>
    <li>
        <p><strong>Compute Partial States in Parallel</strong> — Each chunk is processed in parallel to compute its <strong>partial hidden state</strong>. Each chunk \( C_i \) is processed independently to compute its <strong>partial hidden state</strong> \( H_i \). This is done using the same recurrent update rule, but applied to each chunk in parallel. For example, for chunk \( C_1 \): </p>
            <div class="math-block">
            <div class="equation">
            $$H_1 = f(f(f(f(h_0, x_1), x_2), x_3), x_4), $$ 
        </div>
        </div>
           <p>where \( h_0 \) is the initial hidden state. Similarly, \( H_2 \) is computed for \( C_2 \).</p>
    </li>
    <li>
        <p><strong>Combine the Results</strong> — The partial hidden states \( H_1, H_2, \dots, H_K \) are combined using a <strong>parallel scan</strong> operation. This step ensures that the final hidden state for each token takes into account all previous tokens in the sequence. The parallel scan operation leverages the associative property of \( f \) to efficiently merge the partial states. For example:
        </p>
        <div class="math-block">
            <div class="equation">
            $$ h_4 = H_1, \quad h_8 = f(H_1, H_2).$$
        </div>
        </div>


    </li>
</ol>

<p>
    By leveraging the associative property and parallel scan, Mamba achieves significant speedups over traditional sequential processing, making it highly efficient for long sequences.
</p>

<h4>Why Is This Fast?</h4>
<p>
    By processing chunks in parallel, Mamba avoids the sequential bottleneck of traditional RNNs. This makes it much faster at inference time, especially for long sequences. Here’s why:
</p>
<ul>
    <li><p><strong>Hardware Efficiency</strong> — GPUs are designed to handle parallel computations, and Mamba’s parallel scan algorithm takes full advantage of this capability.</p></li>
    <li><p><strong>Reduced Memory Overhead</strong> — Instead of storing intermediate hidden states for every token, Mamba only needs to store the partial results for each chunk, reducing memory usage.</p></li>
</ul>


    

<h2 id="mamba">5. Mamba Architecture</h2>
<p>
    The <strong>Mamba Block</strong> is designed for efficient sequence modeling. It replaces the quadratic complexity of attention mechanisms with a linear-time approach, combining <strong>State Space Models (SSMs)</strong>, <strong>local feature extraction</strong>, and <strong>input-dependent parameterization</strong>.
</p>

<h3>5.1 Core Components</h3>
<p>
    The Mamba Block consists of the following components:
</p>

<ul>
    <li>
        <p><strong>Root Mean Square Normalization (RMSNorm)</strong>: This normalization stabilizes training by normalizing the input while preserving
            its direction, effectively controlling feature magnitudes.</p>
            <div class="math-block"> 
                <div class="equation">
                $$\hat{x} = \frac{x}{\sqrt{\frac{1}{d} \sum x^2 + \epsilon}}$$
                $$\text{output} = \hat{x} \cdot \gamma $$
            </div>
            </div>   
        <p>where \( \gamma \) is a learnable scaling parameter. This formulation ensures that the normalized output \( \hat{x} \) has unit variance, while the learnable parameter \( \gamma \) 
            allows the model to scale the normalized features appropriately.</p> 
<pre>
<code class="language-python">
class RMSNorm(nn.Module):
    """
    Adapted from:
    - https://github.com/johnma2006/mamba-minimal
    Root Mean Square Layer Normalization (RMSNorm)
    """
    def __init__(self, d_model, eps=1e-5):
        super().__init__()
        self.eps = eps
        self.gamma = nn.Parameter(torch.ones(d_model)) # Learnable Scale
        
    def forward(self, x):
        rms = torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps) # Compute RMS
        return x * rms * self.gamma    
</code> 
</pre>               
    </li>
    <li>
        <p><strong>Local Convolution</strong>: MAMBA uses local convolution before Selective State Space layer which helps in processing 
        local information. This enables mixing of information across tokens, preventing the model from treating tokens independently enabling
    better understanding of the realtionship between nearby tokens. Hence the SSM to recieves an input where the token interactions are 
already partially modeled.</p>
    </li>
<pre>
<code class="language-python">
class LocalConv(nn.Module):
    def __init__(self, d_model, kernel_size=4, conv_bias=True): 
        super().__init__()
        self.conv1d = nn.Conv1d(d_model, d_model, kernel_size, groups=d_model, bias=conv_bias, padding=kernel_size - 1)
    
    def forward(self, x):
        x = x.transpose(1, 2)  # Change to (batch, channels, length) for Conv1D
        x = self.conv1d(x)
        x = x[:, :, :-self.conv1d.kernel_size[0] + 1]  # Adjust shape after padding
        return x.transpose(1, 2)  # Restore original shape (batch, length, channels) 
</code>    
</pre>
    <li>
        <p><strong>Selective State Space Model (SSM)</strong>: The Selective State Space Model is designed to model long-range dependencies by discretizing a continuous-time SSM.
            The hidden state \( h_t \) is given as:</p>
            <div class="math-block">  
            <div class="equation">      
        $$h_t = A_{\Delta} h_{t-1} + B_{\Delta} x_t$$
            </div>
        </div>    
        <p>
            Here, \( A_{\Delta} \) and \( B_{\Delta} \) are discretized versions of the learned matrices \( A \) and \( B \). \( x_t \) is the input at time \( t \).
            Unlike SSM, \( B \) and \( C \) here are <strong>input-dependent</strong>, allowing the model to dynamically focus on relevant parts of the input.
        </p>
<p>
        The output \( y_t \) is computed as:</p>
        <div class="math-block">
        <div class="equation">    
        $$y_t = Ch_t + Dx_t$$
        </div>
    </div>
<pre>
<code class="language-python">
class SelectiveScan(nn.Module):
    """
    Adapted from:
    - https://github.com/johnma2006/mamba-minimal
    Selective Scan module for state-space computation.
    Args:
        u: Input sequence (batch, length, dim)
        dt: Time step scaling (batch, length, dim)
        A: State transition matrix (dim, state_dim)
        B: Input projection matrix (batch, length, state_dim)
        C: Output projection matrix (batch, length, state_dim)
        D: Skip connection (dim)
    Returns:
        Output sequence (batch, length, dim)
    """
    def __init__(self):
        super().__init__()

    def forward(self, u, dt, A, B, C, D):
        # Discretize A: A_Δ = exp(dt * A)
        A_delta = torch.exp(torch.einsum('bld,dn->bldn', dt, A)).clamp(min=-20)
        
        # Input-dependent state update: B_Δ * x_t = dt * u * B
        B_delta_u = torch.einsum('bld,bld,bln->bldn', dt, u, B)
        
        # Cumulative state evolution: cumsum(exp(A_Δ))
        A_delta_cumsum = torch.exp(F.pad(A_delta[:, 1:], (0, 0, 0, 0, 1, 0)).cumsum(1))
        
        # Normalized state: h_t = (B_Δ * x_t) / (cumsum(exp(A_Δ)) + eps)
        h_t = B_delta_u / (A_delta_cumsum + 1e-12)
        
        # Output computation: y_t = C * cumsum(h_t * cumsum(exp(A_Δ)))
        y_t = torch.einsum('bldn,bln->bld', h_t.cumsum(1) * A_delta_cumsum, C)
        
        # Add skip connection: y_t = y_t + D * x_t
        return y_t + u * D
</code>
</pre>       
    </li>
    <li>
        <p><strong>SiLU Activation</strong>: The <strong>SiLU (Sigmoid Linear Unit)</strong> activation function combines a <strong>
            sigmoid gate</strong> with a <strong>linear transformation,</strong>. The SiLU activation is defined as:</p>
            <div class="math-block">
                <div class="equation">
                    $$SiLU(x) = x \cdot \sigma(x)$$
                </div>
            </div>
            
        <p>Here \( x \) is the input, and \( \sigma(x) \) is the sigmoid function.</p> 
    <p>In the Mamba block, SiLU is applied to:</p>
    <ul>
        <li>
            <p>The <strong>convolutional</strong> output to introduce non-linearity.</p>
            <div class="math-block">
                <div class="equation">
                    $$x = SiLU(conv(x))$$
                </div>
            </div>
        </li>
        <li>
            <p>The <strong>residual connection</strong> to gate the skip connection.</p>
            <div class="math-block">
                <div class="equation">
                    $$y = SiLU(res) \cdot SSM(x)$$
                </div>
            </div>
        </li>
    </ul>    
    </li>
    <p>SiLU has a smooth gradient, which helps with stable training.</p>
    <li>
        <p><strong>Residual Connection</strong>: The <strong>Residual Connection</strong> in the Mamba block stabilizes training and
            enables the model to combine information from both the <strong>current token</strong> and the <strong>context computed by the SSM</strong>.
            This allows the model to compute the <strong>similarity</strong> between the context-aware SSM output and the current token's 
            embedding, enabling it to dynamically balance between long-range dependencies and local information.
    </p>
    </li>
</ul>

<h3>5.2 Implementation</h3>
<p>
    The Mamba Block combines input projection, convolutional filtering, and selective-space computation to process
    sequences efficiently. The block is designed to handle long-range dependencies by dynamically focusing on relevant 
    parts of the input.
    
    The key computations in the MAMBA architecture are:
    <ul>
        <li>
            <p><strong>Input Projection:</strong> The input is projected into a higher-dimensional space and split into
            two parts: one for further processing and one for the residual conection.</p>
        </li>
        <li>
            <p><strong>Convolutional Filtering:</strong> A 1D convolution is applied to capture local patterns in the 
            sequence.</p>
        </li>
        <li><p><strong>Selective State-Space Computation:</strong> The input-dependent parameters \( \Delta, B, C \) 
        are computed, and the selective scan mechanim is applied to update the hidden state and compute the output.</li></p>
    <li><p><strong>Output Projection:</strong> The processed sequence is projected back to the original dimension.</li></p>

    </ul>
</p>

<pre>
<code class="language-python">
class MambaBlock(nn.Module):
    """
    Adapted from:
    - https://github.com/johnma2006/mamba-minimal
    Mamba Block: Combines input projection, convolution, and selective state-space computation.
    """
    def __init__(self, d_model, d_state=16, expand=2, dt_rank='auto', d_conv=4, bias=False):
        super().__init__()
        d_inner = expand * d_model
        dt_rank = math.ceil(d_model / 16) if dt_rank == 'auto' else dt_rank
        
        self.in_proj = nn.Linear(d_model, d_inner * 2, bias=bias)
        self.conv = LocalConv(d_inner, d_conv)
        self.x_proj = nn.Linear(d_inner, dt_rank + d_state * 2, bias=False)
        self.dt_proj = nn.Linear(dt_rank, d_inner, bias=True)
        self.A_log = nn.Parameter(torch.log(torch.arange(1, d_state + 1).repeat(d_inner, 1)))
        self.D = nn.Parameter(torch.ones(d_inner))
        self.out_proj = nn.Linear(d_inner, d_model, bias=bias)
        self.scan = SelectiveScan()
        
    def forward(self, x):
        # Input projection and split into x and residual
        x, res = self.in_proj(x).chunk(2, dim=-1)
        
        # Apply 1D convolution
        x = self.conv(x)
        
        # Apply activation and selective scan
        x = F.silu(x)
        y = self.scan(x, * self.compute_params(x))
        
        # Combine with residual and project output
        return self.out_proj(y * F.silu(res))
    
    def compute_params(self, x):
        """
        Compute input-dependent parameters for selective scan.
        Args:
            x: Input tensor (batch, length, dim)
        Returns:
            delta: Time step scaling (batch, length, dim)
            A: State transition matrix (dim, state_dim)
            B: Input projection matrix (batch, length, state_dim)
            C: Output projection matrix (batch, length, state_dim)
            D: Skip connection (dim)
        """
        # Project input to delta, B, and C
        delta, B, C = self.x_proj(x).split([self.dt_proj.in_features, self.A_log.shape[1], self.A_log.shape[1]], dim=-1)
        
        # Compute delta and A
        delta = F.softplus(self.dt_proj(delta))  # Time step scaling
        A = -torch.exp(self.A_log)  # State transition matrix
        
        return delta, A, B, C, self.D
</code>
</pre>

   <h2 id="conclusion">6. Conclusion</h2>
    <p>
        Mamba is a powerful alternative to transformers, offering linear time complexity for sequence modeling. Its selective scanning mechanism dynamically focuses on relevant tokens, making it faster and more memory-efficient than transformers. This efficiency allows Mamba to handle long context lengths effectively, as its context cache (hidden states) does not grow with the sequence length, unlike transformers.</p>
<p> While Mamba shows great promise for scaling to very long sequences, achieving truly infinite context length remains a challenge. However, recent works like Falcon Mamba-7B have introduced interesting strategies to push these boundaries.
</p>

<p> The full code is available to run from this <a href="https://github.com/pranavAL/deep-learning-from-scratch/tree/main/mamba">repository</a>.</p>

    <h2 id="refernce">7. References</h2>
    <ol>
        <li>
            <a href="https://newsletter.maartengrootendorst.com/p/a-visual-guide-to-mamba-and-state">A Visual Guide to Mamba and State Space Models by Maarten Grootendorst</a>
            
        </li>
        <li>
            Gu, Albert, and Tri Dao. "Mamba: Linear-time sequence modeling with selective state spaces." arXiv preprint arXiv:2312.00752 (2023).
        </li>
        <li>
            Code adapted from <a href="https://github.com/johnma2006/mamba-minimal">mamba-minimal</a> and <a href="https://github.com/PeaBrane/mamba-tiny/tree/master"> mamba-tiny</a>, simplified implementations of MAMBA.
        </li>
        <li>
            <a href="https://www.youtube.com/watch?v=vrF3MtGwD0Y&t=806s"> MAMBA and State Space Models explained</a> amazing video by AI Coffee Break with Letitia.
        </li>
        <li>
            Zuo, Jingwei, et al. "Falcon mamba: The first competitive attention-free 7b language model." arXiv preprint arXiv:2410.05355 (2024).
        </li>
    </ol>

    </div>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.28.0/prism.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.28.0/components/prism-python.min.js"></script>

</body>

</html>